# visualizer.py
import streamlit as st
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

def plot_charts(result, model_name="Model", X_test=None, y_test=None):
    """
    –ù—ç–≥ –º–æ–¥–µ–ª–∏–π–Ω “Ø—Ä –¥“Ø–Ω–≥ –Ω–∞—Ä–∏–π–≤—á–ª–∞–Ω —à–∏–Ω–∂–∏–ª–∂, –≥—Ä–∞—Ñ–∏–∫—É—É–¥ –∑—É—Ä–∞—Ö —Ñ—É–Ω–∫—Ü
    """
    model = result['model']
    r2 = result['r2_score']
    mae = result['mae']
    
    # –•—ç—Ä—ç–≤ X_test ”©–≥”©–≥–¥—Å”©–Ω –±–æ–ª —à–∏–Ω—ç—ç—Ä —Ç–∞–∞–º–∞–≥–ª–∞–ª —Ö–∏–π—Ö, “Ø–≥“Ø–π –±–æ–ª —Ö–∞–¥–≥–∞–ª—Å–∞–Ω y_pred-–∏–π–≥ –∞—à–∏–≥–ª–∞—Ö
    y_pred = result.get('y_pred', model.predict(X_test)) if X_test is not None else result['y_pred']

    st.subheader(f"üìä {model_name} Performance") # –ú–æ–¥–µ–ª–∏–π–Ω –Ω—ç—Ä

    # --- –†–ï–ì–†–ï–°–°–ò–ô–ù “Æ–ù–≠–õ–ì–≠–≠ ---
    st.markdown("#### Regression Metrics")
    c1, c2 = st.columns(2)
    c1.metric("R¬≤ Score", f"{r2:.3f}") # –¢–∞–∞–º–∞–≥–ª–∞–ª—ã–Ω –Ω–∞—Ä–∏–π–≤—á–ª–∞–ª (1-–¥ –æ–π—Ä –±–æ–ª —Å–∞–π–Ω)
    c2.metric("MAE (Mean Absolute Error)", f"{mae:.2f}") # –î—É–Ω–¥–∞–∂ –∞–ª–¥–∞–∞ (–ë–∞–≥–∞ –±–æ–ª —Å–∞–π–Ω)

    # --- –ê–ù–ì–ò–õ–õ–´–ù “Æ–ù–≠–õ–ì–≠–≠ (Classification) ---
    # –†–µ–≥—Ä–µ—Å—Å–∏–π–Ω —Ç–æ–æ–Ω “Ø—Ä –¥“Ø–Ω–≥ "–¢—ç–Ω—Ü—Å—ç–Ω/–£–Ω–∞—Å–∞–Ω" –≥—ç—Å—ç–Ω –∞–Ω–≥–∏–ª–∞–ª —Ä—É—É —Ö”©—Ä–≤“Ø“Ø–ª–∂ —à–∞–ª–≥–∞—Ö
    threshold = 70.0 # –¢—ç–Ω—Ü—ç—Ö –±–æ—Å–≥–æ –æ–Ω–æ–æ
    y_test_bin = (y_test >= threshold).astype(int) # –ë–æ–¥–∏—Ç –±–∞–π–¥–∞–ª –¥—ç—ç—Ä —Ç—ç–Ω—Ü—Å—ç–Ω —ç—Å—ç—Ö
    y_pred_bin = (y_pred >= threshold).astype(int) # –¢–∞–∞–º–∞–≥–ª–∞–ª–∞–∞—Ä —Ç—ç–Ω—Ü—Å—ç–Ω —ç—Å—ç—Ö

    # –ê–Ω–≥–∏–ª–ª—ã–Ω —Ö—ç–º–∂“Ø“Ø—Ä“Ø“Ø–¥–∏–π–≥ —Ç–æ–æ—Ü–æ—Ö
    acc = accuracy_score(y_test_bin, y_pred_bin)
    prec = precision_score(y_test_bin, y_pred_bin, zero_division=0)
    rec = recall_score(y_test_bin, y_pred_bin, zero_division=0)
    f1 = f1_score(y_test_bin, y_pred_bin, zero_division=0)

    st.markdown(f"#### Classification Metrics (Pass Threshold: ‚â• {threshold})")
    k1, k2, k3, k4 = st.columns(4)
    k1.metric("Accuracy", f"{acc:.2%}")   # –ù–∏–π—Ç –∑”©–≤ —Ç–∞–∞—Å–∞–Ω —Ö—É–≤—å
    k2.metric("Precision", f"{prec:.2f}") # –¢—ç–Ω—Ü—ç–Ω—ç –≥—ç–∂ —Ç–∞–∞—Å–Ω–∞–∞—Å —Ö—ç–¥ –Ω—å “Ø–Ω—ç—Ö—ç—ç—Ä —Ç—ç–Ω—Ü—Å—ç–Ω –±—ç
    k3.metric("Recall", f"{rec:.2f}")     # –ë–æ–¥–∏—Ç —Ç—ç–Ω—Ü—Å—ç–Ω —Ö“Ø–º“Ø“Ø—Å—ç—ç—Å —Ö—ç–¥–∏–π–≥ –Ω—å –æ–ª–∂ —á–∞–¥—Å–∞–Ω –±—ç
    k4.metric("F1 Score", f"{f1:.2f}")    # Precision, Recall-–∏–π–Ω —Ç—ç–Ω—Ü–≤—ç—Ä–∂“Ø“Ø–ª—Å—ç–Ω –æ–Ω–æ–æ

    # –ì—Ä–∞—Ñ–∏–∫—É—É–¥—ã–≥ 3 ”©”©—Ä —Ü–æ–Ω—Ö–æ–Ω–¥ (Tab) —Ö—É–≤–∞–∞–∂ —Ö–∞—Ä—É—É–ª–∞—Ö
    tab1, tab2, tab3 = st.tabs(["Actual vs Predicted", "Feature Importance", "Detailed Results"])

    with tab1:
        # –ì—Ä–∞—Ñ–∏–∫ 1: –ë–æ–¥–∏—Ç —É—Ç–≥–∞ vs –¢–∞–∞–º–∞–≥–ª–∞—Å–∞–Ω —É—Ç–≥—ã–Ω —Ö–∞—Ä—å—Ü—É—É–ª–∞–ª—Ç (Scatter plot)
        fig, ax = plt.subplots()
        ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', label="Perfect") # –¢”©–≥—Å —à—É–≥–∞–º
        ax.scatter(y_test, y_pred, alpha=0.6)
        ax.set_xlabel("Actual Exam Score")
        ax.set_ylabel("Predicted Exam Score")
        ax.set_title("Prediction Accuracy")
        ax.legend()
        st.pyplot(fig)

        # –ì—Ä–∞—Ñ–∏–∫ 2: –¢”©”©—Ä”©–≥–¥–ª–∏–π–Ω –º–∞—Ç—Ä–∏—Ü (Confusion Matrix) - –¢—ç–Ω—Ü—Å—ç–Ω/–£–Ω–∞—Å–∞–Ω —Ç–∞–∞–º–∞–≥–ª–∞–ª
        cm = confusion_matrix