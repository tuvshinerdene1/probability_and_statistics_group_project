# visualizer.py
import streamlit as st
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

def plot_charts(result, model_name="Model", X_test=None, y_test=None):
    """
    –°–æ–Ω–≥–æ—Å–æ–Ω –Ω—ç–≥ –º–æ–¥–µ–ª–∏–π–Ω “Ø—Ä –¥“Ø–Ω–≥ –Ω–∞—Ä–∏–π–≤—á–ª–∞–Ω —à–∏–Ω–∂–∏–ª–∂, –≥—Ä–∞—Ñ–∏–∫—É—É–¥—ã–≥ –∑—É—Ä–∞—Ö —Ñ—É–Ω–∫—Ü.
    """
    model = result['model']
    r2 = result['r2_score']
    mae = result['mae']
    
    # –•—ç—Ä—ç–≤ y_pred (—Ç–∞–∞–º–∞–≥–ª–∞—Å–∞–Ω —É—Ç–≥–∞) “Ø—Ä –¥“Ø–Ω –¥–æ—Ç–æ—Ä –±–∞–π—Ö–≥“Ø–π –±–æ–ª —à–∏–Ω—ç—ç—Ä —Ç–æ–æ—Ü–æ–æ–ª–æ—Ö
    y_pred = result.get('y_pred', model.predict(X_test)) if X_test is not None else result['y_pred']

    st.subheader(f"üìä {model_name} Performance")

    # --- 1. –†–ï–ì–†–ï–°–°–ò–ô–ù “Æ–ù–≠–õ–ì–≠–≠ ---
    st.markdown("#### Regression Metrics")
    c1, c2 = st.columns(2)
    c1.metric("R¬≤ Score", f"{r2:.3f}") # 1-—Ä“Ø“Ø –¥”©—Ö”©—Ö —Ç—É—Å–∞–º —Å–∞–π–Ω
    c2.metric("MAE (Mean Absolute Error)", f"{mae:.2f}") # –ë–∞–≥–∞ –±–∞–π—Ö —Ç—É—Å–∞–º —Å–∞–π–Ω

    # --- 2. –ê–ù–ì–ò–õ–õ–´–ù “Æ–ù–≠–õ–ì–≠–≠ (Classification) ---
    # –®–∞–ª–≥–∞–ª—Ç—ã–Ω –æ–Ω–æ–æ–≥ –±–æ—Å–≥–æ –æ–Ω–æ–æ (60)-–æ–æ—Ä —Ç–∞—Å–ª–∞–Ω "–¢—ç–Ω—Ü—Å—ç–Ω/–£–Ω–∞—Å–∞–Ω" –≥—ç–∂ –∞–Ω–≥–∏–ª–∞—Ö
    threshold = 60.0
    y_test_bin = (y_test >= threshold).astype(int)
    y_pred_bin = (y_pred >= threshold).astype(int)

    acc = accuracy_score(y_test_bin, y_pred_bin)
    prec = precision_score(y_test_bin, y_pred_bin, zero_division=0)
    rec = recall_score(y_test_bin, y_pred_bin, zero_division=0)
    f1 = f1_score(y_test_bin, y_pred_bin, zero_division=0)

    st.markdown(f"#### Classification Metrics (Pass Threshold: ‚â• {threshold})")
    k1, k2, k3, k4 = st.columns(4)
    k1.metric("Accuracy", f"{acc:.2%}")
    k2.metric("Precision", f"{prec:.2f}")
    k3.metric("Recall", f"{rec:.2f}")
    k4.metric("F1 Score", f"{f1:.2f}")

    # --- 3. –ì–†–ê–§–ò–ö –ë–û–õ–û–ù –î–≠–õ–ì–≠–†–≠–ù–ì“Æ–ô “Æ–† –î“Æ–ù ---
    tab1, tab2, tab3 = st.tabs(["Actual vs Predicted", "Feature Importance", "Detailed Results"])

    with tab1:
        # –ì—Ä–∞—Ñ–∏–∫ 1: –ë–æ–¥–∏—Ç vs –¢–∞–∞–º–∞–≥–ª–∞—Å–∞–Ω —É—Ç–≥—ã–Ω —Ö–∞–º–∞–∞—Ä–∞–ª (Scatter Plot)
        fig, ax = plt.subplots()
        # –¢”©–≥—Å —Ç–∞–∞–º–∞–≥–ª–∞–ª—ã–Ω —à—É–≥–∞–º (–£–ª–∞–∞–Ω —Ç–∞—Å–∞—Ä—Ö–∞–π —à—É–≥–∞–º)
        ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', label="Perfect")
        ax.scatter(y_test, y_pred, alpha=0.6)
        ax.set_xlabel("Actual Exam Score")
        ax.set_ylabel("Predicted Exam Score")
        ax.set_title("Prediction Accuracy")
        ax.legend()
        st.pyplot(fig)

        # –ì—Ä–∞—Ñ–∏–∫ 2: –¢”©”©—Ä”©–≥–¥–ª–∏–π–Ω –º–∞—Ç—Ä–∏—Ü (Confusion Matrix) - –¢—ç–Ω—Ü—Å—ç–Ω/–£–Ω–∞—Å–∞–Ω –±–∞–π–¥–ª–∞–∞—Ä
        cm = confusion_matrix(y_test_bin, y_pred_bin)
        fig, ax = plt.subplots()
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=['Fail <60', 'Pass ‚â•60'],
                    yticklabels=['Fail <60', 'Pass ‚â•60'], ax=ax)
        ax.set_xlabel("Predicted")
        ax.set_ylabel("Actual")
        st.pyplot(fig)

    with tab2:
        # –ê–ª—å —à–∏–Ω–∂ —á–∞–Ω–∞—Ä—É—É–¥ (feature) “Ø—Ä –¥“Ø–Ω–¥ —Ö–∞–º–≥–∏–π–Ω –∏—Ö –Ω”©–ª”©”©–ª—Å”©–Ω –±—ç?
        if hasattr(model, 'feature_importances_'):
            # Decision Tree, Random Forest –≥—ç—Ö –º—ç—Ç –º–æ–¥ —Å—É—É—Ä—å—Ç–∞–π –∑–∞–≥–≤–∞—Ä—É—É–¥
            imp = model.feature_importances_
            feats = X_test.columns
            df_imp = pd.DataFrame({'Feature': feats, 'Importance': imp}).sort_values('Importance', ascending=False).head(10)
            fig, ax = plt.subplots()
            sns.barplot(x='Importance', y='Feature', data=df_imp, ax=ax)
            st.pyplot(fig)
        elif hasattr(model, 'coef_'):
            # Linear Regression –≥—ç—Ö –º—ç—Ç —à—É–≥–∞–º–∞–Ω –∑–∞–≥–≤–∞—Ä—É—É–¥
            imp = np.abs(model.coef_)
            df_imp = pd.DataFrame({'Feature': X_test.columns, 'Importance': imp}).sort_values('Importance', ascending=False).head(10)
            fig, ax = plt.subplots()
            sns.barplot(x='Importance', y='Feature', data=df_imp, ax=ax)
            st.pyplot(fig)
        else:
            st.info("Feature importance not available.")

    with tab3:
        # –ù–∞—Ä–∏–π–≤—á–∏–ª—Å–∞–Ω —Ç–æ–æ–Ω “Ø—Ä –¥“Ø–Ω–≥ —Ö“Ø—Å–Ω—ç–≥—Ç—ç—ç—Ä —Ö–∞—Ä—É—É–ª–∞—Ö
        df_res = pd.DataFrame({
            'Actual': np.round(y_test, 1),
            'Predicted': np.round(y_pred, 1),
            'Error': np.round(np.abs(y_test - y_pred), 1),
            'Status': ['Pass' if x >= threshold else 'Fail' for x in y_test],
            'Pred Status': ['Pass' if x >= threshold else 'Fail' for x in y_pred]
        })
        # –ê–ª–¥–∞–∞ –∏—Ö—Ç—ç–π —Ö—ç—Å–≥“Ø“Ø–¥–∏–π–≥ —É–ª–∞–∞–Ω ”©–Ω–≥”©”©—Ä —Ç–æ–¥—Ä—É—É–ª–∞—Ö
        st.dataframe(df_res.style.background_gradient(cmap='Reds', subset=['Error']), use_container_width=True)

def compare_all_models(results, X_test, y_test):
    """
    –°—É—Ä–≥–∞—Å–∞–Ω –±“Ø—Ö –º–æ–¥–µ–ª–∏—É–¥—ã–Ω “Ø—Ä –¥“Ø–Ω–≥ —Ö–æ–æ—Ä–æ–Ω–¥ –Ω—å —Ö–∞—Ä—å—Ü—É—É–ª–∞—Ö —Ñ—É–Ω–∫—Ü.
    """
    st.subheader("Model Comparison")
    threshold = 60.0
    y_test_bin = (y_test >= threshold).astype(int)

    data = []
    # “Æ—Ä –¥“Ø–Ω –±“Ø—Ä—ç—ç—Å —Ö—ç—Ä—ç–≥—Ç—ç–π –º—ç–¥—ç—ç–ª–ª–∏–π–≥ —Ü—É–≥–ª—É—É–ª–∞—Ö
    for name, res in results.items():
        if 'error' not in res and 'y_pred' in res:
            y_pred = res['y_pred']
            y_pred_bin = (y_pred >= threshold).astype(int)
            data.append({
                'Model': name,
                'R¬≤ Score': res['r2_score'],
                'MAE': res['mae'],
                'Accuracy': accuracy_score(y_test_bin, y_pred_bin),
                'F1 Score': f1_score(y_test_bin, y_pred_bin, zero_division=0)
            })

    if data:
        # “Æ—Ä –¥“Ø–Ω–≥ —ç—Ä—ç–º–±—ç–ª–∂ —Ö–∞—Ä—É—É–ª–∞—Ö
        df = pd.DataFrame(data).sort_values('R¬≤ Score', ascending=False)
        st.dataframe(df.style.format({'R¬≤ Score': '{:.3f}', 'MAE': '{:.2f}', 'Accuracy': '{:.2%}', 'F1 Score': '{:.3f}'})
                     .background_gradient(cmap='Greens', subset=['R¬≤ Score', 'Accuracy']), use_container_width=True)

        col1, col2 = st.columns(2)
        with col1:
            # R2 –æ–Ω–æ–æ–Ω—ã —Ö–∞—Ä—å—Ü—É—É–ª–∞–ª—Ç (–ì—Ä–∞—Ñ–∏–∫)
            fig, ax = plt.subplots()
            sns.barplot(x='R¬≤ Score', y='Model', data=df, ax=ax)
            ax.set_title("R¬≤ Score Comparison")
            st.pyplot(fig)
        with col2:
            # –ù–∞—Ä–∏–π–≤—á–ª–∞–ª—ã–Ω —Ö–∞—Ä—å—Ü—É—É–ª–∞–ª—Ç (–ì—Ä–∞—Ñ–∏–∫)
            fig, ax = plt.subplots()
            sns.barplot(x='Accuracy', y='Model', data=df, ax=ax)
            ax.set_title("Accuracy (‚â•60 = Pass)")
            st.pyplot(fig)