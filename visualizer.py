import streamlit as st
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

def plot_charts(result, model_name="Model", X_test=None, y_test=None):
    """
    –ù—ç–≥ –∑–∞–≥–≤–∞—Ä—ã–Ω “Ø—Ä –¥“Ø–Ω–≥ –Ω–∞—Ä–∏–π–≤—á–ª–∞–Ω —à–∏–Ω–∂–∏–ª–∂, –≥—Ä–∞—Ñ–∏–∫ –±–æ–ª–æ–Ω —Ç–æ–æ–Ω “Ø–∑“Ø“Ø–ª—ç–ª—Ç—ç—ç—Ä —Ö–∞—Ä—É—É–ª–∞—Ö —Ñ—É–Ω–∫—Ü.
    """
    # –ó–∞–≥–≤–∞—Ä –±–æ–ª–æ–Ω “Ø–Ω—ç–ª–≥—ç—ç–Ω“Ø“Ø–¥–∏–π–≥ –∑–∞–¥–ª–∞–∂ –∞–≤–∞—Ö
    model = result['model']
    r2 = result['r2_score']
    mae = result['mae']
    # –•—ç—Ä—ç–≤ y_pred —Ö–∞–¥–≥–∞–ª–∞–≥–¥–∞–∞–≥“Ø–π –±–æ–ª –¥–∞—Ö–∏–Ω —Ç–∞–∞–º–∞–≥–ª–∞–ª —Ö–∏–π—Ö
    y_pred = result.get('y_pred', model.predict(X_test))
    
    st.subheader(f"üìä {model_name} Performance")
    
    # 1. –†–µ–≥—Ä–µ—Å—Å–∏–π–Ω “Ø–Ω–¥—Å—ç–Ω “Ø–Ω—ç–ª–≥—ç—ç–Ω“Ø“Ø–¥ (–¢–æ–æ–Ω —Ç–∞–∞–º–∞–≥–ª–∞–ª)
    st.markdown("#### üî¢ Regression Metrics")
    c1, c2 = st.columns(2)
    c1.metric("R¬≤ Score (Fit)", f"{r2:.2f}")  # –ó–∞–≥–≤–∞—Ä—ã–Ω ”©–≥”©–≥–¥”©–ª–¥ —Ç–æ—Ö–∏—Ä—Å–æ–Ω –±–∞–π–¥–∞–ª (1-–¥ –æ–π—Ä –±–æ–ª —Å–∞–π–Ω)
    c2.metric("Mean Error (MAE)", f"{mae:.2f}") # –î—É–Ω–¥–∞–∂ –∞–ª–¥–∞–∞ (0-–¥ –æ–π—Ä –±–æ–ª —Å–∞–π–Ω)
    
    # 2. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–π–Ω “Ø–Ω—ç–ª–≥—ç—ç–Ω“Ø“Ø–¥ (–ë–æ—Å–≥–æ –æ–Ω–æ–æ–≥–æ–æ—Ä –∞–Ω–≥–∏–ª–∞—Ö)
    st.markdown("#### üö¶ Classification Metrics (Threshold: GPA ‚â• 3.0)")
    st.caption("Converting continuous GPA predictions to Binary (Good vs. Low) to calculate Accuracy/F1.")
    
    # GPA 3.0-–æ–æ—Å –¥—ç—ç—à –±–æ–ª "–°–∞–π–Ω", –¥–æ–æ—à –±–æ–ª "–ú—É—É" –≥—ç–∂ “Ø–∑—ç–∂ —Ö–æ—ë—Ä—Ç—ã–Ω –∞–Ω–≥–∏–ª–∞–ª —Ö–∏–π—Ö
    threshold = 3.0
    
    y_test_bin = (y_test >= threshold).astype(int) # –ë–æ–¥–∏—Ç —É—Ç–≥—ã–≥ 0, 1 –±–æ–ª–≥–æ—Ö
    y_pred_bin = (y_pred >= threshold).astype(int) # –¢–∞–∞–º–∞–≥–ª–∞—Å–∞–Ω —É—Ç–≥—ã–≥ 0, 1 –±–æ–ª–≥–æ—Ö
    
    # –ê–Ω–≥–∏–ª–ª—ã–Ω “Ø–Ω—ç–ª–≥—ç—ç–Ω“Ø“Ø–¥–∏–π–≥ —Ç–æ–æ—Ü–æ—Ö
    acc = accuracy_score(y_test_bin, y_pred_bin)
    prec = precision_score(y_test_bin, y_pred_bin, zero_division=0)
    rec = recall_score(y_test_bin, y_pred_bin, zero_division=0)
    f1 = f1_score(y_test_bin, y_pred_bin, zero_division=0)
    
    # “Æ—Ä –¥“Ø–Ω–≥ —Ö–∞—Ä—É—É–ª–∞—Ö
    k1, k2, k3, k4 = st.columns(4)
    k1.metric("Accuracy", f"{acc:.2%}")
    k2.metric("Precision", f"{prec:.2f}")
    k3.metric("Recall", f"{rec:.2f}")
    k4.metric("F1 Score", f"{f1:.2f}")

    # 3. –ì—Ä–∞—Ñ–∏–∫ –±–æ–ª–æ–Ω –¥—ç–ª–≥—ç—Ä—ç–Ω–≥“Ø–π –º—ç–¥—ç—ç–ª–ª–∏–π–Ω Tab-—É—É–¥
    tab1, tab2, tab3 = st.tabs(["Actual vs Predicted", "Feature Importance", "Posterior Table"])
    
    # Tab 1: –ë–æ–¥–∏—Ç vs –¢–∞–∞–º–∞–≥–ª–∞–ª
    with tab1:
        # Scatter plot –∑—É—Ä–∞—Ö
        fig, ax = plt.subplots(figsize=(6, 4))
        ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label="Perfect Prediction")
        ax.scatter(y_test, y_pred, alpha=0.7, color='blue')
        ax.set_xlabel("Actual GPA")
        ax.set_ylabel("Predicted GPA")
        ax.set_title("Regression Fit")
        ax.legend()
        st.pyplot(fig)
        
        st.markdown("---")
        st.markdown("**Confusion Matrix (based on Threshold)**")
        # –¢”©”©—Ä”©–≥–¥–ª–∏–π–Ω –º–∞—Ç—Ä–∏—Ü (Confusion Matrix) –∑—É—Ä–∞—Ö
        cm = confusion_matrix(y_test_bin, y_pred_bin)
        fig_cm, ax_cm = plt.subplots(figsize=(4, 3))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax_cm,
                    xticklabels=['< 3.0', '>= 3.0'], yticklabels=['< 3.0', '>= 3.0'])
        ax_cm.set_xlabel("Predicted")
        ax_cm.set_ylabel("Actual")
        st.pyplot(fig_cm)

    # Tab 2: –®–∏–Ω–∂ —á–∞–Ω–∞—Ä—ã–Ω —á—É—Ö–∞–ª –±–∞–π–¥–∞–ª (Feature Importance)
    with tab2:
        # –ú–æ–¥ (Tree) —Å—É—É—Ä—å—Ç–∞–π –∑–∞–≥–≤–∞—Ä—É—É–¥ (Random Forest, Decision Tree –≥—ç—Ö –º—ç—Ç)
        if hasattr(model, 'feature_importances_') and X_test is not None:
            importances = model.feature_importances_
            feature_names = X_test.columns
            feat_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})
            # –•–∞–º–≥–∏–π–Ω —á—É—Ö–∞–ª 10 —à–∏–Ω–∂ —á–∞–Ω–∞—Ä—ã–≥ —ç—Ä—ç–º–±—ç–ª—ç—Ö
            feat_df = feat_df.sort_values(by='Importance', ascending=False).head(10)
            
            fig_feat, ax_feat = plt.subplots(figsize=(8, 5))
            sns.barplot(x='Importance', y='Feature', data=feat_df, palette='viridis', ax=ax_feat)
            ax_feat.set_title("Feature Importance")
            st.pyplot(fig_feat)
        
        # –®—É–≥–∞–º–∞–Ω (Linear) –∑–∞–≥–≤–∞—Ä—É—É–¥ (Linear Regression –≥—ç—Ö –º—ç—Ç)
        elif hasattr(model, 'coef_'):
            importances = model.coef_
            feature_names = X_test.columns
            feat_df = pd.DataFrame({'Feature': feature_names, 'Importance': np.abs(importances)})
            feat_df = feat_df.sort_values(by='Importance', ascending=False).head(10)
            
            fig_feat, ax_feat = plt.subplots(figsize=(8, 5))
            sns.barplot(x='Importance', y='Feature', data=feat_df, palette='magma', ax=ax_feat)
            st.pyplot(fig_feat)
        else:
            st.info("Feature importance not available for this model type.")

    # Tab 3: –î—ç–ª–≥—ç—Ä—ç–Ω–≥“Ø–π “Ø—Ä –¥“Ø–Ω–≥–∏–π–Ω —Ö“Ø—Å–Ω—ç–≥—Ç
    with tab3:
        st.markdown("### üìã Detailed Predictions")
        
        results_df = pd.DataFrame({
            'Actual GPA': y_test.values if hasattr(y_test, 'values') else y_test,
            'Predicted GPA': np.round(y_pred, 2),
            'Error (Diff)': np.round(np.abs(y_test - y_pred), 2),
            'Status (Act)': ['High' if x >= threshold else 'Low' for x in y_test],
            'Status (Pred)': ['High' if x >= threshold else 'Low' for x in y_pred]
        })
        
        # –ê–ª–¥–∞–∞ –∏—Ö—Ç—ç–π –º”©—Ä“Ø“Ø–¥–∏–π–≥ —Ç–æ–¥—Ä—É—É–ª–∂ —Ö–∞—Ä—É—É–ª–∞—Ö
        st.dataframe(
            results_df.style.background_gradient(cmap='Reds', subset=['Error (Diff)']),
            use_container_width=True
        )

def compare_all_models(results, y_test_data=None):
    """
    –°—É—Ä–≥–∞—Å–∞–Ω –±“Ø—Ö –∑–∞–≥–≤–∞—Ä—É—É–¥—ã–Ω “Ø—Ä –¥“Ø–Ω–≥ —Ö–æ–æ—Ä–æ–Ω–¥ –Ω—å —Ö–∞—Ä—å—Ü—É—É–ª–∞—Ö —Ñ—É–Ω–∫—Ü.
    Note: y_test_data –Ω—å —Ö–∞—Ä—å—Ü—É—É–ª–∞–ª—Ç —Ö–∏–π—Ö—ç–¥ —à–∞–∞—Ä–¥–ª–∞–≥–∞—Ç–∞–π –±–æ–¥–∏—Ç —É—Ç–≥—É—É–¥.
    """
    st.subheader("üìà Model Comparison")
    
    comparison_data = []
    threshold = 3.0 # –•–∞—Ä—å—Ü—É—É–ª–∞—Ö–∞–¥ –∞—à–∏–≥–ª–∞—Ö –±–æ—Å–≥–æ –æ–Ω–æ–æ
    
    # –•—ç—Ä—ç–≤ y_test_data —Ñ—É–Ω–∫—Ü—ç–¥ –¥–∞–º–∂—É—É–ª–∞–≥–¥–∞–∞–≥“Ø–π –±–æ–ª —ç—Ö–Ω–∏–π –∑–∞–≥–≤–∞—Ä–∞–∞—Å –∞–≤–∞—Ö –æ—Ä–æ–ª–¥–ª–æ–≥–æ —Ö–∏–π—Ö
    if y_test_data is None:
        # –≠–Ω—ç –Ω—å –∞–ª–¥–∞–∞ –≥–∞—Ä–∞—Ö–∞–∞—Å —Å—ç—Ä–≥–∏–π–ª—Å—ç–Ω –Ω—ç–º—ç–ª—Ç —à–∞–ª–≥–∞–ª—Ç —é–º
        first_key = list(results.keys())[0]
        if 'y_test' in results[first_key]:
            y_test_data = results[first_key]['y_test']

    for name, result in results.items():
        if 'error' not in result:
            model = result['model']
            if 'y_pred' in result:
                y_p = result['y_pred']
            else:
                continue 
            
            # –†–µ–≥—Ä–µ—Å—Å–∏–π–Ω —É—Ç–≥—ã–≥ –∞–Ω–≥–∏–ª–∞–ª —Ä—É—É —Ö”©—Ä–≤“Ø“Ø–ª—ç—Ö
            y_t_bin = (y_test_data >= threshold).astype(int)
            y_p_bin = (y_p >= threshold).astype(int)
            
            # –•–∞—Ä—å—Ü—É—É–ª–∞—Ö ”©–≥”©–≥–¥–ª“Ø“Ø–¥–∏–π–≥ –∂–∞–≥—Å–∞–∞–ª—Ç–∞–¥ –Ω—ç–º—ç—Ö
            comparison_data.append({
                'Model': name,
                'R2 Score': result['r2_score'],
                'MAE (Error)': result['mae'],
                'Accuracy': accuracy_score(y_t_bin, y_p_bin),
                'F1 Score': f1_score(y_t_bin, y_p_bin, zero_division=0)
            })
    
    if comparison_data:
        # R2 –æ–Ω–æ–æ–≥–æ–æ—Ä —ç—Ä—ç–º–±—ç–ª—ç—Ö
        df_comparison = pd.DataFrame(comparison_data).sort_values('R2 Score', ascending=False)
        
        # –•“Ø—Å–Ω—ç–≥—Ç–∏–π–≥ ”©–Ω–≥”©”©—Ä —è–ª–≥–∞–∂ —Ö–∞—Ä—É—É–ª–∞—Ö
        st.dataframe(
            df_comparison.style.format("{:.2f}").background_gradient(cmap='Greens', subset=['R2 Score', 'Accuracy', 'F1 Score']),
            use_container_width=True
        )
        
        # –•–∞—Ä—å—Ü—É—É–ª—Å–∞–Ω –≥—Ä–∞—Ñ–∏–∫—É—É–¥—ã–≥ –∑—É—Ä–∞—Ö
        col1, col2 = st.columns(2)
        with col1:
            fig, ax = plt.subplots(figsize=(6, 4))
            sns.barplot(x='R2 Score', y='Model', data=df_comparison, palette='viridis', ax=ax)
            ax.set_xlim(0, 1.0)
            ax.set_title("R¬≤ Comparison (Higher is Better)")
            st.pyplot(fig)
        with col2:
            fig, ax = plt.subplots(figsize=(6, 4))
            sns.barplot(x='Accuracy', y='Model', data=df_comparison, palette='magma', ax=ax)
            ax.set_xlim(0, 1.0)
            ax.set_title(f"Accuracy Comparison (GPA >= {threshold})")
            st.pyplot(fig)